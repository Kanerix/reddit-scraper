{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from typing import Iterable\n",
    "\n",
    "import polars as pl\n",
    "from httpx import HTTPError, HTTPTransport\n",
    "from swiftshadow.classes import ProxyInterface\n",
    "\n",
    "from scrp.client import RateLimitError, RedditScraper\n",
    "from scrp.model import ChildrenT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY_MANAGER = ProxyInterface(protocol=\"http\", autoRotate=True, autoUpdate=False)\n",
    "await PROXY_MANAGER.async_update()\n",
    "\n",
    "def get_scraper():\n",
    "    proxy = PROXY_MANAGER.get()\n",
    "    print(f\"using new proxy '{proxy.as_string()}'\")\n",
    "\n",
    "    return RedditScraper(\n",
    "        mounts={\n",
    "            f\"{proxy.protocol}://\": HTTPTransport(\n",
    "                proxy=proxy.as_string(),\n",
    "            ),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_fail(attempt: int, delay: int | None = None) -> RedditScraper | None:\n",
    "    if delay is None:\n",
    "        delay = 60\n",
    "\n",
    "    if attempt > 5:\n",
    "        return None\n",
    "\n",
    "    sleep(delay)\n",
    "\n",
    "    return get_scraper()\n",
    "\n",
    "\n",
    "def scrape_comments(permalinks: Iterable[str]) -> pl.DataFrame:\n",
    "    scraper = get_scraper()\n",
    "\n",
    "    retry_attempts = 0\n",
    "    after, df = None, None\n",
    "\n",
    "    for permalink in permalinks:\n",
    "        try:\n",
    "            search = scraper.comments(permalink, limit=10)\n",
    "        except RateLimitError as e:\n",
    "            scraper = on_fail(retry_attempts, delay=e.retry_after)\n",
    "            retry_attempts += 1\n",
    "            if scraper is None:\n",
    "                print(\"still getting rate limit errors after 5 retries.\")\n",
    "                break\n",
    "            continue\n",
    "        except HTTPError:\n",
    "            scraper = on_fail(retry_attempts)\n",
    "            retry_attempts += 1\n",
    "            if scraper is None:\n",
    "                print(\"still getting errors after 5 retries.\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        comments = search[1]\n",
    "        data = [\n",
    "            children.data\n",
    "            for children in comments.data.children\n",
    "            if isinstance(children, ChildrenT1)\n",
    "        ]\n",
    "\n",
    "        if len(data) == 0:\n",
    "            scraper = on_fail(retry_attempts)\n",
    "            retry_attempts += 1\n",
    "            if scraper is None:\n",
    "                print(\"stopped receiving data after 5 retries.\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        retry_attempts = 0\n",
    "\n",
    "        if df is None:\n",
    "            df = pl.DataFrame(data)\n",
    "        else:\n",
    "            df = df.vstack(pl.DataFrame(data))\n",
    "\n",
    "        print(f\"scraped {len(data)} ({df.height}) rows for permalink '{permalink}'.\")\n",
    "\n",
    "        after = comments.data.after\n",
    "        if after is None:\n",
    "            print(\"no more data.\")\n",
    "            break\n",
    "    \n",
    "    if df is None:\n",
    "        raise RuntimeError(\"no data was scraped.\")\n",
    "\n",
    "    print(f\"finished scraping {df.height} posts for term '{permalink}'.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"python_1000\"\n",
    "\n",
    "path = Path(f\"output/{file}.parquet\".lower())\n",
    "\n",
    "if path.exists():\n",
    "    df = pl.read_parquet(path)\n",
    "else:\n",
    "    print(\"failed to read parquet, file does not exist.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permalinks = df.select(\"permalink\").to_series()\n",
    "df = scrape_comments(permalinks)\n",
    "\n",
    "path = Path(f\"output/{file}_comments.parquet\".lower())\n",
    "\n",
    "if not path.exists():\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        df.write_parquet(path)\n",
    "else:\n",
    "    print(\"failed to write parquet, file already exists.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
