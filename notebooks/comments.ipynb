{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from typing import Iterable\n",
    "\n",
    "import polars as pl\n",
    "from httpx import HTTPError, HTTPTransport\n",
    "from swiftshadow.classes import ProxyInterface\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    ")\n",
    "\n",
    "from scrp.client import RedditScraper\n",
    "from scrp.model import ChildrenT1, RedditListing, RedditChildren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY_MANAGER = ProxyInterface(protocol=\"http\", autoRotate=True, autoUpdate=False)\n",
    "await PROXY_MANAGER.async_update()\n",
    "\n",
    "\n",
    "def get_scraper():\n",
    "    proxy = PROXY_MANAGER.get()\n",
    "    print(f\"using new proxy '{proxy.as_string()}'\")\n",
    "\n",
    "    return RedditScraper(\n",
    "        mounts={\n",
    "            f\"{proxy.protocol}://\": HTTPTransport(\n",
    "                proxy=proxy.as_string(),\n",
    "            ),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnest(comments: list[RedditChildren]) -> list[ChildrenT1]:\n",
    "    replies = []\n",
    "    no_replies = 0\n",
    "\n",
    "    while True:\n",
    "        for children in comments:\n",
    "            if not isinstance(children, ChildrenT1) or children.data.replies is None:\n",
    "                no_replies += 1\n",
    "                continue\n",
    "            replies.append(children.data.replies)\n",
    "\n",
    "        comments = replies\n",
    "        replies = []\n",
    "\n",
    "        if len(comments) == no_replies:\n",
    "            break\n",
    "\n",
    "    return replies\n",
    "\n",
    "\n",
    "def on_fail(attempt: int, delay: int, reason: str) -> RedditScraper:\n",
    "    if attempt > 5:\n",
    "        raise RuntimeError(f\"Failed to scrape comments; {reason}\")\n",
    "    sleep(delay)\n",
    "    return get_scraper()\n",
    "\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(2),\n",
    "    wait=wait_exponential(multiplier=4),\n",
    "    retry=retry_if_exception_type(HTTPError),\n",
    "    before_sleep=lambda _: print(\"Failed to scrape comments; retrying...\"),\n",
    "    reraise=True,\n",
    ")\n",
    "def try_scrape_comments(\n",
    "    scraper: RedditScraper, permalink: str\n",
    ") -> list[RedditListing] | None:\n",
    "    try:\n",
    "        return scraper.comments(permalink, limit=100, sort=\"controversial\")\n",
    "    except HTTPError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape(permalinks: Iterable[str]) -> pl.DataFrame:\n",
    "    permalinks = iter(permalinks)\n",
    "    permalink = next(permalinks, None)\n",
    "\n",
    "    scraper = get_scraper()\n",
    "\n",
    "    retry_attempts = 0\n",
    "\n",
    "    while permalink and retry_attempts < 5:\n",
    "        response = try_scrape_comments(scraper, permalink)\n",
    "        if response is None:\n",
    "            scraper = on_fail(retry_attempts, 60, \"HTTPError\")\n",
    "            retry_attempts += 1\n",
    "            sleep(30 * retry_attempts)\n",
    "            continue\n",
    "\n",
    "        comments = unnest(response[1].data.children)\n",
    "\n",
    "        if len(comments) == 0:\n",
    "            scraper = on_fail(retry_attempts, 60, \"API returned no data\")\n",
    "            retry_attempts += 1\n",
    "            sleep(30 * retry_attempts)\n",
    "            continue\n",
    "\n",
    "        retry_attempts = 0\n",
    "\n",
    "        if df is None:\n",
    "            df = pl.DataFrame(comments).drop(\"replies\")\n",
    "        else:\n",
    "            df = pl.DataFrame(comments).drop(\"replies\").vstack(df)\n",
    "\n",
    "        short = permalink.split(\"/\")[4]\n",
    "        print(f\"scraped {len(comments)} ({df.height}) rows for permalink '{short}'.\")\n",
    "\n",
    "    if df is None:\n",
    "        raise RuntimeError(\"No data was scraped.\")\n",
    "\n",
    "    if retry_attempts >= 5:\n",
    "        print(\"Failed to scrape comments; exited early...\")\n",
    "\n",
    "    print(f\"Finished scraping {df.height} comments for '{permalink}'.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"combined\"\n",
    "path = Path(f\"output/{file}.parquet\".lower())\n",
    "\n",
    "if path.exists():\n",
    "    df_posts = pl.read_parquet(path)\n",
    "else:\n",
    "    raise ValueError(\"failed to read parquet, file does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permalinks = df_posts.select(\"permalink\").to_series()\n",
    "df = scrape(permalinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(f\"output/{file}_comments.parquet\".lower())\n",
    "\n",
    "if not path.exists():\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        df.write_parquet(path)\n",
    "else:\n",
    "    print(\"failed to write parquet, file already exists.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
