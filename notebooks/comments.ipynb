{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from typing import Iterable\n",
    "\n",
    "import polars as pl\n",
    "from httpx import HTTPError, HTTPTransport\n",
    "from swiftshadow.classes import ProxyInterface\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    ")\n",
    "\n",
    "from scrp.client import RedditScraper\n",
    "from scrp.model import ChildT1, DataT1, RedditChild, RedditListing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY_MANAGER = ProxyInterface(protocol=\"http\", autoRotate=True, autoUpdate=False)\n",
    "await PROXY_MANAGER.async_update()\n",
    "\n",
    "\n",
    "def get_scraper():\n",
    "    proxy = PROXY_MANAGER.get()\n",
    "    print(f\"Using new proxy '{proxy.as_string()}'\")\n",
    "\n",
    "    return RedditScraper(\n",
    "        mounts={\n",
    "            f\"{proxy.protocol}://\": HTTPTransport(\n",
    "                proxy=proxy.as_string(),\n",
    "            ),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnest(comments: RedditListing) -> list[DataT1]:\n",
    "    all_comments: list[DataT1] = []\n",
    "    queue = deque(comments.data.children)\n",
    "\n",
    "    while queue:\n",
    "        comment = queue.popleft()\n",
    "        if isinstance(comment, ChildT1):\n",
    "            if replies := comment.data.replies:\n",
    "                queue.extend(replies.data.children)\n",
    "            all_comments.append(comment.data)\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "\n",
    "def on_fail(attempt: int, reason: str) -> RedditScraper:\n",
    "    if attempt > 5:\n",
    "        raise RuntimeError(f\"Failed to scrape comments; {reason}\")\n",
    "    print(f\"Getting new scraper; {reason}\")\n",
    "    return get_scraper()\n",
    "\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(2),\n",
    "    wait=wait_exponential(multiplier=4),\n",
    "    retry=retry_if_exception_type(HTTPError),\n",
    "    before_sleep=lambda _: print(\"Failed to scrape comments; retrying...\"),\n",
    "    reraise=True,\n",
    ")\n",
    "def try_scrape_comments(\n",
    "    scraper: RedditScraper, permalink: str\n",
    ") -> list[RedditListing] | None:\n",
    "    try:\n",
    "        return scraper.comments(permalink, limit=100, sort=\"controversial\")\n",
    "    except HTTPError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape(permalinks: Iterable[str]) -> pl.DataFrame:\n",
    "    permalinks = iter(permalinks)\n",
    "    permalink = next(permalinks, None)\n",
    "\n",
    "    scraper = get_scraper()\n",
    "\n",
    "    df, retry_attempts, count = None, 0, 0\n",
    "\n",
    "    while permalink and retry_attempts < 5:\n",
    "        response = try_scrape_comments(scraper, permalink)\n",
    "        if response is None:\n",
    "            scraper = on_fail(retry_attempts, \"API returned an HTTPError\")\n",
    "            retry_attempts += 1\n",
    "            sleep(30 * retry_attempts)\n",
    "            continue\n",
    "\n",
    "        retry_attempts = 0\n",
    "        count += 1\n",
    "\n",
    "        comments = unnest(response[1])\n",
    "\n",
    "        if len(comments) == 0:\n",
    "            permalink = next(permalinks, None)\n",
    "            continue\n",
    "\n",
    "        if df is None:\n",
    "            df = pl.DataFrame(comments).drop(\"replies\")\n",
    "        else:\n",
    "            df = pl.DataFrame(comments).drop(\"replies\").vstack(df)\n",
    "\n",
    "        short = permalink.split(\"/\")[4]\n",
    "        print(f\"Scraped {len(comments)} ({df.height}) rows for permalink '{short}' (Count: {count}).\")\n",
    "        permalink = next(permalinks, None)\n",
    "\n",
    "    if df is None:\n",
    "        raise RuntimeError(\"No data was scraped.\")\n",
    "\n",
    "    if retry_attempts >= 5:\n",
    "        print(\"Failed to scrape all comments; exited early...\")\n",
    "\n",
    "    print(f\"Finished scraping {df.height} comments for '{permalink}'.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"output/combined.parquet\")\n",
    "\n",
    "if path.exists():\n",
    "    df_posts = pl.read_parquet(path)\n",
    "    df_posts = df_posts.unique(\"id\")\n",
    "else:\n",
    "    raise ValueError(\"Failed to read parquet, file does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permalinks = df_posts.select(\"permalink\").to_series()\n",
    "df = scrape(permalinks)\n",
    "# df = df.vstack(pl.read_parquet(\"output/combined_comments.parquet\")).unique(\"id\") -> To extend to dataframe with new comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"output/combined_comments.parquet\")\n",
    "\n",
    "if not path.exists():\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        df.write_parquet(path)\n",
    "else:\n",
    "    print(\"Failed to write parquet, file already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last = pl.read_parquet(\"output/combined_comments.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
