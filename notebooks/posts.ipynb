{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "from httpx import HTTPError, HTTPTransport\n",
    "from swiftshadow.classes import ProxyInterface\n",
    "\n",
    "from scrp.client import RedditScraper, RateLimitError\n",
    "from scrp.model import ChildrenT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY_MANAGER = ProxyInterface(protocol=\"http\", autoRotate=True, autoUpdate=False)\n",
    "await PROXY_MANAGER.async_update()\n",
    "\n",
    "def get_scraper():\n",
    "    proxy = PROXY_MANAGER.get()\n",
    "    print(f\"using new proxy '{proxy.as_string()}'\")\n",
    "\n",
    "    return RedditScraper(\n",
    "        mounts={\n",
    "            f\"{proxy.protocol}://\": HTTPTransport(\n",
    "                proxy=proxy.as_string(),\n",
    "            ),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_fail(attempt: int, delay: int | None = None) -> RedditScraper | None:\n",
    "    if delay is None:\n",
    "        delay = 60\n",
    "\n",
    "    if attempt > 5:\n",
    "        return None\n",
    "\n",
    "    sleep(delay)\n",
    "\n",
    "    return get_scraper()\n",
    "\n",
    "\n",
    "def scrape_posts(term: str, posts_in_hundreds: int = 500) -> pl.DataFrame:\n",
    "    scraper = get_scraper()\n",
    "\n",
    "    retry_attempts = 0\n",
    "    after, df = None, None\n",
    "\n",
    "    for _ in range(posts_in_hundreds):\n",
    "        try:\n",
    "            search = scraper.search(term, limit=100, after=None, show=\"all\")\n",
    "        except RateLimitError as e:\n",
    "            scraper = on_fail(retry_attempts, delay=e.retry_after)\n",
    "            retry_attempts += 1\n",
    "            print(f\"getting rate limit errors after {retry_attempts} retries.\")\n",
    "            if scraper is None:\n",
    "                break\n",
    "            continue\n",
    "        except HTTPError:\n",
    "            scraper = on_fail(retry_attempts)\n",
    "            retry_attempts += 1\n",
    "            print(f\"getting errors after {retry_attempts} retries.\")\n",
    "            if scraper is None:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        data = [\n",
    "            children.data\n",
    "            for children in search.data.children\n",
    "            if isinstance(children, ChildrenT3)\n",
    "        ]\n",
    "\n",
    "        if len(data) == 0:\n",
    "            scraper = on_fail(retry_attempts)\n",
    "            retry_attempts += 1\n",
    "            print(f\"stopped receiving data after {retry_attempts} retries.\")\n",
    "            if scraper is None:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        retry_attempts = 0\n",
    "\n",
    "        if df is None:\n",
    "            df = pl.DataFrame(data)\n",
    "        else:\n",
    "            df = df.vstack(pl.DataFrame(data))\n",
    "\n",
    "        print(f\"scraped {len(data)} ({df.height}) rows for term '{term}'.\")\n",
    "\n",
    "        after = search.data.after\n",
    "        if after is None:\n",
    "            print(\"no more data.\")\n",
    "            break\n",
    "\n",
    "    if df is None:\n",
    "        raise RuntimeError(\"no data was scraped.\")\n",
    "\n",
    "    print(f\"finished scraping {df.height} posts for term '{term}'.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TERMS = [\n",
    "    \"Asylansøgere\",\n",
    "    \"Flygtninge\",\n",
    "    \"Indvandrere\",\n",
    "    \"Migranter\",\n",
    "    \"Udlændinge\",\n",
    "]\n",
    "\n",
    "for term in SEARCH_TERMS:\n",
    "    df = scrape_posts(term, posts_in_hundreds=500)\n",
    "    df = df.with_columns(search_term=pl.lit(term))\n",
    "\n",
    "    path = Path(f\"output/{term}_{df.height}.parquet\".lower())\n",
    "\n",
    "    if not path.exists():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\") as f:\n",
    "            df.write_parquet(path)\n",
    "    else:\n",
    "        print(\"failed to write parquet, file already exists.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
