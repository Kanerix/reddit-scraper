{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "import polars as pl\n",
    "from httpx import HTTPError, HTTPTransport\n",
    "from swiftshadow.classes import ProxyInterface\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    ")\n",
    "\n",
    "from scrp.client import RedditScraper\n",
    "from scrp.model import ChildrenT3, RedditListing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY_MANAGER = ProxyInterface(protocol=\"http\", autoRotate=True, autoUpdate=False)\n",
    "await PROXY_MANAGER.async_update()\n",
    "\n",
    "\n",
    "def get_scraper():\n",
    "    proxy = PROXY_MANAGER.get()\n",
    "    print(f\"using new proxy '{proxy.as_string()}'\")\n",
    "\n",
    "    return RedditScraper(\n",
    "        mounts={\n",
    "            f\"{proxy.protocol}://\": HTTPTransport(\n",
    "                proxy=proxy.as_string(),\n",
    "            ),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_fail(attempt: int, reason: str) -> RedditScraper:\n",
    "    if attempt > 5:\n",
    "        raise RuntimeError(f\"Failed to scrape comments; {reason}\")\n",
    "    print(f\"Getting new scraper; {reason}\")\n",
    "    return get_scraper()\n",
    "\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(2),\n",
    "    wait=wait_exponential(multiplier=4),\n",
    "    retry=retry_if_exception_type(HTTPError),\n",
    "    before_sleep=lambda _: print(\"Failed to scrape comments; retrying...\"),\n",
    "    reraise=True,\n",
    ")\n",
    "def try_scrape_comments(scraper: RedditScraper, term: str) -> RedditListing | None:\n",
    "    try:\n",
    "        return scraper.search(term, limit=100, after=None, show=\"all\")\n",
    "    except HTTPError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape(term: str, posts_in_hundreds: int = 500) -> pl.DataFrame:\n",
    "    scraper = get_scraper()\n",
    "\n",
    "    retry_attempts = 0\n",
    "    after, df = None, None\n",
    "\n",
    "    while posts_in_hundreds & retry_attempts < 5:\n",
    "        response = try_scrape_comments(scraper, term)\n",
    "        if response is None:\n",
    "            scraper = on_fail(retry_attempts, \"HTTPError\")\n",
    "            retry_attempts += 1\n",
    "            sleep(30 * retry_attempts)\n",
    "            continue\n",
    "\n",
    "        data = [\n",
    "            child.data\n",
    "            for child in response.data.children\n",
    "            if isinstance(child, ChildrenT3)\n",
    "        ]\n",
    "\n",
    "        if len(data) == 0:\n",
    "            scraper = on_fail(retry_attempts, \"API returned no data\")\n",
    "            retry_attempts += 1\n",
    "            sleep(30 * retry_attempts)\n",
    "            continue\n",
    "\n",
    "        if df is None:\n",
    "            df = pl.DataFrame(data)\n",
    "        else:\n",
    "            df = pl.DataFrame(data).vstack(df)\n",
    "\n",
    "        print(f\"Scraped {len(data)} ({df.height}) rows for term '{term}'.\")\n",
    "\n",
    "        after = response.data.after\n",
    "        if after is None:\n",
    "            print(\"Pagination stopped.\")\n",
    "            break\n",
    "\n",
    "        posts_in_hundreds -= 1\n",
    "\n",
    "    if df is None:\n",
    "        raise RuntimeError(\"No data was scraped.\")\n",
    "\n",
    "    if retry_attempts >= 5:\n",
    "        print(\"Failed to scrape comments; exited early...\")\n",
    "\n",
    "    print(f\"finished scraping {df.height} posts for term '{term}'.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TERMS = [\n",
    "    \"Anden etnicitet\",\n",
    "    \"Asylansøgere\",\n",
    "    \"Asylpolitik\",\n",
    "    \"Blackface i Danmark\",\n",
    "    \"Flygtninge\",\n",
    "    \"Ghettoloven\",\n",
    "    \"Hyggeracisme\",\n",
    "    \"Indvandrere\",\n",
    "    \"Islam i Danmark\",\n",
    "    \"Islamisme\",\n",
    "    \"Migranter\",\n",
    "    \"Migrantkrise\",\n",
    "    \"Udlændigepolitik\",\n",
    "    \"Udlændinge\",\n",
    "    \"Ulovlig invandring\",\n",
    "    \"Parallelsamfund\",\n",
    "    \"Perkere\",\n",
    "    \"Racisme\",\n",
    "    \"Racismeparagraffen\",\n",
    "    \"Statsborgerskab\",\n",
    "    \"Strukturel racisme\",\n",
    "]\n",
    "\n",
    "for term in SEARCH_TERMS:\n",
    "    df = scrape(term, posts_in_hundreds=50)\n",
    "    df = df.with_columns(search_term=pl.lit(term))\n",
    "\n",
    "    path = Path(f\"output/{term}_{df.height}.parquet\".lower().replace(\" \", \"_\"))\n",
    "\n",
    "    if not path.exists():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\") as f:\n",
    "            df.write_parquet(path)\n",
    "    else:\n",
    "        print(\"failed to write parquet, file already exists.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
